**SYS 6018 | Fall 2020 | University of Virginia **

*******************************************

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 5,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "49%", # set width of displayed images
                      warning=FALSE,     # do not show R warnings
                      message=FALSE)     # do not show R messages
options(dplyr.summarise.inform = FALSE)  # ignore message about group structure
```

<!--- Solution Region --->
```{css solution-region, echo=FALSE}
.solution {
  background-color: #232D4B10;
  border-style: solid;
  border-color: #232D4B;
  padding: .5em;
  margin: 20px
}
```


<!--- Load Required R packages here --->
```{r formatting, include=FALSE}
#- Better table printing
library(kableExtra) # https://haozhu233.github.io/kableExtra/awesome_table_in_html.html
format_table <- function(x, nmax=10) {
  kable(x) %>% 
    kable_styling(full_width = FALSE, font_size=11, position = "left") %>% 
    {if(nrow(x) > nmax) scroll_box(., width = "100%", height = "200px") else .}
}
#- useful functions
digits <- function(x, k=2) format(round(x, k), nsmall=k)
#- data directory
data.dir = 'https://mdporter.github.io/SYS6018/data/'
#- required functions
library(tidyverse)
library(glmnet)
library(glmnetUtils)  # to allow formula interface in glmnet()
library(ISLR)
library(caret)
library(FNN)
library(broom)
library(yardstick)
library(tidyverse) 
library(parallel)
library(caret)
```



## Crime Linkage

Crime linkage attempts to determine if two or more unsolved crimes share a common offender. *Pairwise* crime linkage is the more simple task of deciding if two crimes share a common offender; it can be considered a binary classification problem. The linkage training data has 8 evidence variables that measure the similarity between a pair of crimes:

- `spatial` is the spatial distance between the crimes
- `temporal` is the fractional time (in days) between the crimes
- `tod` and `dow` are the differences in time of day and day of week between the crimes
- `LOC`, `POA,` and `MOA` are binary with a 1 corresponding to a match (type of property, point of entry, method of entry)
- `TIMERANGE` is the time between the earliest and latest possible times the crime could have occurred (because the victim was away from the house during the crime).
- The response variable indicates if the crimes are linked ($y=1$) or unlinked ($y=0$).


These problems use the [linkage-train](https://mdporter.github.io/SYS6018/data/linkage_train.csv) and [linkage-test](https://mdporter.github.io/SYS6018/data/linkage_test.csv) datasets (click on links for data). 


### Problem 4.1: Penalized Regression for Crime Linkage

a. Fit a penalized *linear regression* model. Use a lasso, ridge, or elasticnet penalty (your choice). 
    - Report the value of $\alpha$ used (if elasticnet)
    - Report the value of $\lambda$ used
    - Report the estimated coefficients


<div class="solution"> 

```{r}
train = read.csv(file = 'linkage_train.csv')
test = read.csv(file = 'linkage_test.csv')

X.train = as.matrix(train[,-9])
Y.train = as.matrix(train$y)

```


```{r}
set.seed(123456789)
fold = sample(rep(1:10, length=nrow(X.train)))
Linear = cv.glmnet(X.train, Y.train, alpha=0.8, foldid=fold)
Linear$lambda.min
#broom::tidy(Linear) 
coef(Linear, s="lambda.min")

```
alpha = 0.2
lambda = 0.0003314119
</div>


b. Fit a penalized *logistic regression* model. Use a lasso, ridge, or elasticnet penalty (your choice).  
    - Report the value of $\alpha$ used (if elasticnet)
    - Report the value of $\lambda$ used
    - Report the estimated coefficients

<div class="solution"> 
```{r}
set.seed(123456789)
fit.enet = cv.glmnet(y~., data=train, 
                     alpha=.5,
                     family="binomial")

broom::tidy(fit.enet) 
fit.enet$lambda.min
coef(fit.enet, s="lambda.min")
```
alpha = 0.5
lambda = 0.00006911937e
</div>

c. Produce one plot that has the ROC curves, using the *training data*, for both models (from part a and b). Use color and/or linetype to distinguish between models and include a legend.    

<div class="solution"> 

```{r}
set.seed(123456789)

## Using yardstick package

#---------------------------------------------------------------------------#
#-- Performance Metrics and Curves
#---------------------------------------------------------------------------#
#-- train/test split

#-- fit model on training data

#-- Get predictions (of p(x) and gamma(x)) on test data
p.hat = predict(Linear,newx = X.train, newdata=train, type='response')  
gamma = predict(Linear,newx = X.train,newdata=train, type='link')  

#-- Make Hard classification (use .20 as cut-off)
G.hat = ifelse(p.hat >= .20, 1, 0)

#-- Make Confusion Table
G.test = train$y  # true values
table(predicted=G.hat, truth = G.test) %>% addmargins()

#-- Get performance data (by threshold)
#   This table has one row for every threshold. The columns are the elements
#   of the confusion table plus FPR, TPR
perf = tibble(truth = G.test, gamma, p.hat) %>% 
  #- group_by() + summarize() in case of ties
  group_by(gamma, p.hat) %>%     
  summarize(n=n(), n.1=sum(truth), n.0=n-sum(truth)) %>% ungroup() %>% 
  #- calculate metrics
  arrange(gamma) %>% 
  mutate(FN = cumsum(n.1),    # false negatives 
         TN = cumsum(n.0),    # true negatives
         TP = sum(n.1) - FN,  # true positives
         FP = sum(n.0) - TN,  # false positives
         N = cumsum(n),       # number of cases predicted to be 1
         TPR = TP/sum(n.1), FPR = FP/sum(n.0)) %>% 
  #- only keep relevant metrics
  select(-n, -n.1, -n.0, gamma, p.hat)

perf$ROC <- 'linear_model'

#-- fit model on training data
#-- Get predictions (of p(x) and gamma(x)) on test data
p.hat = predict(fit.enet, newdata=train, type='response')  
gamma = predict(fit.enet, newdata=train, type='link')  

#-- Make Hard classification (use .10 as cut-off)
G.hat = ifelse(p.hat >= .10, 1, 0)

#-- Make Confusion Table
G.test = train$y  # true values
table(predicted=G.hat, truth = G.test) %>% addmargins()

#-- Get performance data (by threshold)
#   This table has one row for every threshold. The columns are the elements
#   of the confusion table plus FPR, TPR
perf_1 = tibble(truth = G.test, gamma, p.hat) %>% 
  #- group_by() + summarize() in case of ties
  group_by(gamma, p.hat) %>%     
  summarize(n=n(), n.1=sum(truth), n.0=n-sum(truth)) %>% ungroup() %>% 
  #- calculate metrics
  arrange(gamma) %>% 
  mutate(FN = cumsum(n.1),    # false negatives 
         TN = cumsum(n.0),    # true negatives
         TP = sum(n.1) - FN,  # true positives
         FP = sum(n.0) - TN,  # false positives
         N = cumsum(n),       # number of cases predicted to be 1
         TPR = TP/sum(n.1), FPR = FP/sum(n.0)) %>% 
  #- only keep relevant metrics
  select(-n, -n.1, -n.0, gamma, p.hat)

perf_1$ROC <- 'logistic_model'

ROC_1 <-  rbind(perf,perf_1)

ROC_1 %>% 
  ggplot(aes(FPR, TPR,group = ROC)) + geom_path() + geom_line(alpha = 1, aes(color = ROC, group = ROC)) +
  labs(x='FPR (1-specificity)', y='TPR (sensitivity)') + 
  geom_segment(x=0, xend=1, y=0, yend=1, lty=3, color='grey20') + 
  scale_x_continuous(breaks = seq(0, 1, by=.20)) + 
  scale_y_continuous(breaks = seq(0, 1, by=.20)) + 
  ggtitle("ROC Curve")
```




</div>


d. Recreate the ROC curve from the penalized logistic regression model using repeated  . The following steps will guide you:
    - Fix $\alpha=.75$ 
    - Run the following steps 25 times:
    i. Hold out 500 observations
    ii. Use the remaining observations to estimate $\lambda$ 
    iii. Predict the probability of the 500 hold-out observations
    iv. Store the predictions and hold-out labels
    - Combine the results and produce the hold-out based ROC curve
    - Note: by estimating $\lambda$ each iteration, we are incorporating the uncertainty present in estimating that tuning parameter. 
    
<div class="solution"> 
```{r}
set.seed(123456789)
#create 2 new df to store the predictions and hold-out labels
probability <- data.frame(probability=numeric(0), 
                          holdout=numeric(0))

Gamma <-  data.frame(ytest=numeric(0), gamma=numeric(0))

for(i in 1:25) {  # 25 simulation
sample_500 <-  sample(nrow(train), 500)    

X.train = train[-sample_500,] %>% 
  select(-y) %>% 
  as.matrix()
Y.train = train[-sample_500,] %>% 
  select(y) %>% 
  as.matrix()

fit.log = cv.glmnet(X.train, Y.train, #penalized logistic regression model
                    alpha=.75,
                    family="binomial")

X.test = train[sample_500,] %>% 
  select(-y) %>% 
  as.matrix()

# prediction
p.hat = predict(fit.log, sample_500="lambda.min", newx=X.test, type="response")
gamma <- predict(fit.log, sample_500="lambda.min", newx=X.test, type="link")

# change to numeric
gamma = as.numeric(gamma)
# change to dataframe type 
df_p <- tibble(p.hat)
# add hold-out 
df_p$holdout <- i

Y.test = train[sample_500,] %>% 
  select(y) %>% 
  as.matrix()

df_gamma <-  tibble(ytest = Y.test, gamma = gamma)
Gamma <- rbind(Gamma, df_gamma )
probability <- rbind(probability, df_p)
}

Gamma$gamma <- as.vector(Gamma$gamma)
Gamma$ytest <- as.factor(Gamma$ytest)
combined_25_500 = yardstick::roc_curve(Gamma, ytest,gamma, event_level = 'second')

autoplot(combined_25_500)  

```


</div>


e. Contest Part 1: Predict the estimated *probability* of linkage for the test data (using any model). 
    - Submit a .csv file (ensure comma separated format) named `lastname_firstname_1.csv` that includes the column named **p** that is your estimated posterior probability. We will use automated evaluation, so the format must be exact. 
    - You are free to use any tuning parameters
    - You are free to use any data transformation or feature engineering
    - You will receive credit for a proper submission; the top five scores will receive 2 bonus points.     
    - Your probabilities will be evaluated with respect to the mean negative Bernoulli log-likelihood (known as the average *log-loss* metric)
$$ 
L = - \frac{1}{M} \sum_{i=1}^m [y_i \log \, \hat{p}_i + (1 - y_i) \log \, (1 - \hat{p}_i)]
$$
where $M$ is the number of test observations, $\hat{p}_i$ is the prediction for the $i$th test observation, and $y_i \in \{0,1\}$ are the true test set labels. 

<div class="solution"> 
```{r}
set.seed(1234567890)

Probability_e = predict(Linear, newx = X.test, newdata=train, s=0.0003314119, type='response') 
Probability_e = as.numeric(Probability_e)

# Hard classification 
G.hat = ifelse(Probability_e >= .2, 1, 0)

# add column
df_e <- cbind(X.test, Probability_e)
colnames(df_e )[9] <- 'p'


write.csv(as.data.frame(df_e) , "Chen_keyu_1.csv",row.names=FALSE)

```

</div>
 

f. Contest Part 2: Predict the linkages for the test data (using any model). 
    - Submit a .csv file (ensure comma separated format) named `lastname_firstname_2.csv` that includes the column named **link** that takes the value of 1 for linkages and 0 for unlinked pairs. We will use automated evaluation, so the format must be exact. 
    - You are free to use any tuning parameters.
    - You are free to use any data transformation or feature engineering.
    - Your labels will be evaluated based on total cost, where cost is equal to `1*FP + 8*FN`. This implies that False Negatives (FN) are 8 times as costly as False Negatives (FP)    
    - You will receive credit for a proper submission; the top five scores will receive 2 bonus points. Note: you only will get bonus credit for one of the two contests. 

<div class="solution"> 

```{r}

df_f <- cbind(X.test, G.hat)
colnames(df_f)[9] <- 'link'

write.csv(as.data.frame(df_f), "Chen_Keyu_2.csv",row.names=FALSE)
```

</div>






